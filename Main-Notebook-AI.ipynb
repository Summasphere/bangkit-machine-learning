{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gOJNx4EwCY4",
        "outputId": "b288a119-afb7-43b4-85b6-bbd9ac04d6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras_nlp\n",
            "  Downloading keras_nlp-0.12.1-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2024.5.15)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.2.5)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (4.66.4)\n",
            "Collecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Collecting tensorflow<2.17,>=2.16.1 (from tensorflow-text->keras_nlp)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Collecting h5py (from keras-core->keras_nlp)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (4.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (1.64.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.37.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.43.0)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras_nlp) (2.1.5)\n",
            "Installing collected packages: namex, PyPDF2, optree, ml-dtypes, h5py, tensorboard, keras-core, keras, tensorflow, tensorflow-text, keras_nlp\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 h5py-3.11.0 keras-3.3.3 keras-core-0.1.7 keras_nlp-0.12.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-text-2.16.1\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2 keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### GLOBAL UTILITY FUNCTION #####\n",
        "import ast\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def sanitize_text(text: str) -> str:\n",
        "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
        "\n",
        "def string_to_object(string:str):\n",
        "    return ast.literal_eval(string.strip(\";\"))\n",
        "\n",
        "def sanitize_text(text: str) -> str:\n",
        "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
        "\n",
        "def string_to_object(string:str):\n",
        "    return ast.literal_eval(string.strip(\";\"))\n",
        "\n",
        "def fetch_html(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def parse_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def extract_info(soup):\n",
        "    # Contoh: Ekstrak teks dari semua paragraf\n",
        "    txt_Arr = []\n",
        "    paragraphs = soup.find_all('p')\n",
        "    for para in paragraphs:\n",
        "        txt_Arr.append(para.get_text())\n",
        "\n",
        "    return txt_Arr\n",
        "\n",
        "def process_url(url):\n",
        "    html_content = fetch_html(url)\n",
        "    if html_content:\n",
        "        soup = parse_html(html_content)\n",
        "        text = extract_info(soup)\n",
        "        text = \" \".join(text)\n",
        "        return text\n",
        "##### END-GLOBAL UTILITY FUNCTION #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdv30SQqvJ_Y",
        "outputId": "8b5b918d-037c-47e6-93d0-68a01e949319"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "############## IMPORT NECESSARY LIBRARIES ###############\n",
        "\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "import io\n",
        "import PyPDF2\n",
        "import re\n",
        "from wordcloud import wordcloud\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "\n",
        "import time\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import random\n",
        "\n",
        "################## DEFINE THE ENV VARIABLES #####################\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "os.environ[\"HF_TOKEN\"] = 'ENTER_YOUR_HUGGINGFACE_TOKEN'\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"ENTER_YOUR_NGROK_TOKEN\"\n",
        "\n",
        "\n",
        "################## LOAD CONFIGURATIONS #####################\n",
        "with open(\"./config.yaml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "url = \"https://huggingface.co/mnabielap/bart-multinews/resolve/main/bart-multinews.keras\"\n",
        "local_path = \"bart-multinews.keras\"\n",
        "os.system(f\"wget {url} -O {local_path}\")\n",
        "\n",
        "############## DEFINE GLOBAL POINTER FOR API ##############\n",
        "POINTER = 0\n",
        "GEMINI_API_KEY_COLLECTION = config[\"GEMINI_API_KEY_COLLECTION\"]\n",
        "random.shuffle(GEMINI_API_KEY_COLLECTION)\n",
        "\n",
        "################### SUMMARIZER CLASS ############################\n",
        "class BartSummarizer: # /summarize/bart\n",
        "    def __init__(self, max_length = 256):\n",
        "        self.max_length = max_length\n",
        "        self.bart_model = tf.keras.models.load_model(\n",
        "            local_path,\n",
        "            custom_objects={\"BartSeq2SeqLM\": keras_nlp.models.BartSeq2SeqLM}\n",
        "        )\n",
        "\n",
        "    def summarize(self, input_text):\n",
        "        output = self.bart_model.generate(input_text, max_length=self.max_length)\n",
        "        return output\n",
        "\n",
        "class GeminiLLM():\n",
        "    def __init__(self, configs=config):\n",
        "        self.maximum_try = 10\n",
        "        self.api_key = GEMINI_API_KEY_COLLECTION\n",
        "        self.generation_conf = configs[\"generation_config\"]\n",
        "        self.safety_settings = [\n",
        "            {\n",
        "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "                \"threshold\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "            },\n",
        "            {\n",
        "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "                \"threshold\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "            },\n",
        "            {\n",
        "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "                \"threshold\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "            },\n",
        "            {\n",
        "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                \"threshold\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    def pick_random_key(self):\n",
        "        global POINTER\n",
        "        if self.api_key:\n",
        "            pair_api_key = self.api_key[POINTER]\n",
        "            POINTER = (POINTER + 1) % len(self.api_key) # move to the next\n",
        "            api_key, email_name = pair_api_key\n",
        "            print(f\"Using API Key from -> {email_name}\")\n",
        "            return api_key\n",
        "        else:\n",
        "            return \"No more API keys available.\"\n",
        "\n",
        "    def generate_result(self, input_text, system_instruction):\n",
        "        input_text = sanitize_text(input_text)\n",
        "        api_key = self.pick_random_key()\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        model = genai.GenerativeModel(\n",
        "            model_name=\"gemini-1.5-flash\",\n",
        "            safety_settings=self.safety_settings,\n",
        "            generation_config=self.generation_conf,\n",
        "            system_instruction=system_instruction,\n",
        "        )\n",
        "\n",
        "        chat_session = model.start_chat(history=[])\n",
        "        response = chat_session.send_message(input_text)\n",
        "\n",
        "        response_text = response.text\n",
        "        return response_text\n",
        "\n",
        "    ######## UTILITY FUNCTION ########\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def create_str_json_example(self, num_topic: int):\n",
        "        res = \"[\\n\"\n",
        "        for no_topic in range(num_topic):\n",
        "            no_topic += 1\n",
        "            comma = ',' if no_topic != num_topic else ''\n",
        "            temp = '''\n",
        "            {\n",
        "                \"topic\":\"{name_topic_'''.strip() + str(no_topic) + '''}\",\n",
        "                \"percentage\":{percentage_'''.strip() + str(no_topic) + '''},\n",
        "                \"detail\":\"{explanation_'''.strip() + str(no_topic) + '''}\"\n",
        "            }\n",
        "            '''.strip() + str(comma)\n",
        "            res += f\"  {temp}\\n\"\n",
        "        res += \"]\"\n",
        "        return res\n",
        "    ##################################\n",
        "\n",
        "class GeminiSummarizer(GeminiLLM): # /summarize/gemini\n",
        "    def __init__(self, configs=config):\n",
        "        super().__init__(configs)\n",
        "\n",
        "    def process_text(self, input_text):\n",
        "        system_instruction='Objective:\\nYou are summarization application specifically designed for researchers to efficiently extract key information from academic papers. Researchers often face the challenge of sifting through numerous papers to find inspiration and relevant information, but only about 20% of a paper typically contains the critical insights they need. This application aims to expedite the literature review process by providing concise, targeted summaries focusing on the most valuable parts of each paper.\\n\\nKey Requirements:\\n\\nMethodology Summary: Clearly outline the research methods used, including experimental design, data collection, and analysis techniques.\\n\\nEquations: Highlight and extract every important equation if exists. The equations must be written in LaTeX so it can be rendered in markdown media. Do not let more than three equations on the same line, if there are more than three, put it in the new line. Make sure to always use the equation environment to write an equation that is given in a line, e.g. $$ H_ {k_p}=frac {Y_ {k_p}} {X_ {k_p}} $$. Also, make sure to be careful on writing equations from documents provided by user, because sometimes PDF breaks the latex format and you might write it wrong. Make sure to not forget every detail, for example you must write it like\\n\\n$$ A_{dot} = \\\\text{softmax} (  \\n\\\\frac {QK^T} {\\\\sqrt{d_{model}}}  \\n) V. $$  \\n$$ A_{mem} = \\\\frac{σ(Q)M_{s-1}}\\n{σ(Q)z_{s−1}} . $$  \\n$$ M_{s} ← M_{s−1} + σ(K)^TV \\\\text{ and } z_{s} ← z_{s−1} + \\\\sum_{t=1}^{N} \\nσ(K_{t}). $$  \\n$$ M_{s} ← M_{s−1} + σ(K)^T(V − \\\\frac{σ(K)M_{s−1}}{σ(K)z_{s−1}}). $$\\n\\nMake sure to do deeper reasoning to implement the equation correctly, I know you render the equations from PDF directly, but use your knowledge to figure out how is it supposed to be written correctly. Do not just write what you saw directly.\\n\\nResults Summary: Highlight the main findings and outcomes of the research, emphasizing significant results and conclusions.\\nCitations for Each Argument: Provide citations AND paper reference in APA style in the end of the summary, for key arguments and claims made within the paper to facilitate further reading and verification. The citation must be written in APA style, extract from the Reference Section in the input document if exists. REMEMBER, just provide the some needed citations and reference only, no need to provide all of the citations used on the paper.\\n\\nImportant Aspects of the Method: Identify and summarize critical aspects and innovations of the methodology that contribute to the research field.\\n\\nApplication Expectations:\\n\\nNon-Generic Summaries: The application should avoid general summaries (such as abstracts) and focus on specific sections that contain essential details for researchers.\\nEfficiency and Accuracy: Ensure that the summarization process is fast and accurate, enabling researchers to quickly grasp the core contributions of each paper.\\nUser-Centric Design: Tailor the application interface and features to meet the needs of researchers, allowing them to customize the type and depth of summaries they receive.\\nOutcome:\\nBy using you, researchers should be able to significantly reduce the time spent on literature reviews, thereby enhancing their productivity and enabling them to produce more research papers. The application should act as a valuable tool in accelerating the research process and improving the overall quality of academic work.\\n\\nYou are not allowed to answer another question aside summarization task. Expected responses:\\n\\nExample 1:\\nUsers: \"Hello\"\\nYou: <No response>\\n\\nExample 2:\\nUsers: \"Umm\"\\nYou: <No response>\\n'\n",
        "        response_text = self.generate_result(input_text, system_instruction)\n",
        "        return response_text\n",
        "\n",
        "    def run_gemini_summarizer(self, text, mode=\"text\"):\n",
        "        if mode == \"pdf\":\n",
        "            text = self.extract_text_from_pdf(text)\n",
        "\n",
        "        for _ in range(self.maximum_try):\n",
        "            try:\n",
        "                summary = self.process_text(text)\n",
        "                return summary\n",
        "            except Exception as e:\n",
        "                print(f\"error: {e}\")\n",
        "        return \"Failed to summarize the text. Please try again later.\"\n",
        "\n",
        "################### ANALYZER CLASS ############################\n",
        "class TopicModelling(GeminiLLM):\n",
        "    def __init__(self, configs=config):\n",
        "        super().__init__(configs)\n",
        "\n",
        "    def process_text(self, input_text):\n",
        "        system_instruction=f\"\"\"\n",
        "            Analyze the 5 most related topics in the text below. explain each topic completely. explain why the text fits the topic.\n",
        "            >> Constraint:\n",
        "            - minimum 100 words and maximum 500 words\n",
        "            - also explain how many percent of each topic is related to the text.\n",
        "            - provide in a formal, no-nonsense format so that these results can be used for academic purposes.\n",
        "            - to the point\n",
        "            - the output must be json format for example\n",
        "            {self.create_str_json_example(5)}\n",
        "            - do not provide any text outside of json\n",
        "            \"\"\".strip()\n",
        "        response_text = self.generate_result(input_text, system_instruction)\n",
        "        return response_text\n",
        "\n",
        "    def wordcloud(self, text):\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = text.split()\n",
        "        words = [word for word in words if len(word) > 2]\n",
        "\n",
        "        additional_stopwords = {'could', 'would', 'never', 'one', 'even', 'like', 'said', 'say', 'also',\n",
        "                                'might', 'must', 'every', 'much', 'may', 'two', 'know', 'upon', 'without',\n",
        "                                'go', 'went', 'got', 'put', 'see', 'seem', 'seemed', 'take', 'taken',\n",
        "                                'make', 'made', 'come', 'came', 'look', 'looking', 'think', 'thinking',\n",
        "                                'thought', 'use', 'used', 'find', 'found', 'give', 'given', 'tell', 'told',\n",
        "                                'ask', 'asked', 'back', 'get', 'getting', 'keep', 'kept', 'let', 'lets',\n",
        "                                'seems', 'leave', 'left', 'set', 'from', 'subject', 're', 'edu', 'use'}\n",
        "        custom_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
        "\n",
        "        filtered_words = [word for word in words if word not in custom_stopwords]\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "        word_freq = Counter(lemmatized_words)\n",
        "        sorted_word_freq = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
        "        temp_dict = dict()\n",
        "        for key, val in sorted_word_freq:\n",
        "            temp_dict[key] = val\n",
        "        return temp_dict\n",
        "\n",
        "    ######################## ANDROID UTILITY ########################\n",
        "    def barplot_to_base64(self, data):\n",
        "        # Extracting data for plotting\n",
        "        topics = [item['topic'] for item in data]\n",
        "        percentages = [item['percentage'] for item in data]\n",
        "\n",
        "        # Creating the barplot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(topics, percentages, color='skyblue')\n",
        "        plt.xlabel('Percentage')\n",
        "        plt.ylabel('Topic')\n",
        "        plt.title('Distribution of Topics in Quantum Computing')\n",
        "        plt.gca().invert_yaxis()  # Invert y-axis to have the highest percentage on top\n",
        "\n",
        "        # Save the plot to a BytesIO object\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "\n",
        "        # Encode the BytesIO object to base64\n",
        "        img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        buf.close()\n",
        "\n",
        "        return img_base64\n",
        "\n",
        "    def wordcloud_to_base64(self, word_freq):\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        plt.close()\n",
        "        buffer.seek(0)\n",
        "        img_str = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        return img_str\n",
        "    ########################################################################\n",
        "\n",
        "    def run_analysis(self, text, mode=\"pdf\", media=\"frontend\"):\n",
        "        if mode == \"pdf\":\n",
        "            text = self.extract_text_from_pdf(text)\n",
        "        elif mode == \"link\":\n",
        "            text = process_url(text)\n",
        "\n",
        "        for _ in range(self.maximum_try):\n",
        "            try:\n",
        "                topic_dist = self.process_text(text)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"error: {e}\")\n",
        "        try:\n",
        "            topic_dist = string_to_object(topic_dist[topic_dist.index(\"[\"):topic_dist.rindex(\"]\")+1])\n",
        "        except Exception:\n",
        "            topic_dist = string_to_object(topic_dist[topic_dist.index(\"{\"):topic_dist.rindex(\"}\")+1])\n",
        "\n",
        "        wordcloud_dict = self.wordcloud(text)\n",
        "\n",
        "        if mode == \"android\":\n",
        "            topic_dist_img = self.barplot_to_base64(topic_dist)\n",
        "            wordcloud_img = self.wordcloud_to_base64(wordcloud_dict)\n",
        "            dict_analysis = {\"topic_distribution\": topic_dist_img, \"wordcloud\": wordcloud_img}\n",
        "        else:\n",
        "            dict_analysis = {\"topic_distribution\": topic_dist, \"wordcloud\": wordcloud_dict}\n",
        "\n",
        "        return dict_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jVaKMU8rw6oS"
      },
      "outputs": [],
      "source": [
        "summarizer_bart = BartSummarizer()\n",
        "summarizer_gemini = GeminiSummarizer()\n",
        "topic_modeller = TopicModelling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LDm9XL8_0_mK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In the year 2023, the small coastal town of Cape Serenity was known for its breathtaking landscapes, historic lighthouse, and vibrant community. However, what truly set Cape Serenity apart was its annual \"Festival of Lights,\" an event that drew visitors from all over the world. This festival, held every December, transformed the town into a glowing wonderland, with every house, shop, and street decorated with millions of twinkling lights.\n",
        "\n",
        "The origins of the Festival of Lights date back to the late 19th century. It all began when a local fisherman, Thomas Branson, decided to decorate his boat with lanterns to guide him back home through the dense fog that often enveloped the coast during winter. His initiative inspired other fishermen, and soon, the entire fleet was adorned with lights. This impromptu tradition quickly gained popularity among the townspeople, and by the turn of the century, it had evolved into a full-fledged festival.\n",
        "\n",
        "Over the decades, the festival grew in scale and splendor. The town council took charge of organizing the event, ensuring that each year was more spectacular than the last. They introduced various attractions, including light shows, parades, and fireworks. Local businesses and residents eagerly participated, competing for the most beautifully decorated property, a coveted title that came with a significant cash prize and local fame.\n",
        "\n",
        "The highlight of the festival was always the grand parade. Floats elaborately decorated with lights and themes from different cultures and stories made their way through the town's main street. The parade was not only a visual feast but also a showcase of Cape Serenity's diversity and unity. Each float was designed by a different community group, from the local schools and churches to the town's various cultural societies.\n",
        "\n",
        "In 2023, the Festival of Lights was particularly special. It marked the 125th anniversary of the event, and the town council spared no effort in making it the grandest celebration yet. Months before December, preparations were already underway. Volunteers worked tirelessly, hanging lights, setting up displays, and coordinating events. The local children, excited about the upcoming festivities, practiced for their roles in the parade and various performances.\n",
        "\n",
        "One of the most anticipated additions to the festival was the interactive light garden in the town square. This installation featured thousands of LED flowers that responded to touch and movement, creating an ever-changing sea of color and light. Visitors could walk through the garden, their steps triggering waves of illumination that followed them. It was a hit among both the young and the old, providing countless photo opportunities and moments of awe.\n",
        "\n",
        "As December approached, the town buzzed with anticipation. The local bakery, known for its seasonal treats, unveiled a new creation: the \"Lighthouse Cookie,\" a gingerbread replica of Cape Serenity's iconic lighthouse, complete with edible lights. The cookie quickly became a bestseller, with people lining up to get a taste.\n",
        "\n",
        "The festival officially began on the first Friday of December, with the lighting ceremony at the town square. The mayor, accompanied by the oldest and youngest residents of Cape Serenity, flipped the switch that set the town aglow. The moment was magical, with the lights twinkling to life against the backdrop of a clear, starry night. Cheers erupted, and the festivities commenced.\n",
        "\n",
        "Throughout the month, Cape Serenity was a hive of activity. Tourists flooded in, filling up the local inns and restaurants. The town's economy received a significant boost, and the sense of community pride was palpable. There were concerts, street performances, and workshops where visitors could learn to make their own lanterns and decorations.\n",
        "\n",
        "The grand parade, held on the third Saturday of December, was the crowning jewel of the festival. This year's theme was \"Journey Through Time,\" celebrating the history of the festival and the town. Floats depicted scenes from different eras, from the humble beginnings with Thomas Branson's lantern-lit boat to futuristic visions of Cape Serenity. The creativity and effort put into each float were evident, and the parade drew record crowds.\n",
        "\n",
        "As the festival drew to a close on New Year's Eve, there was a sense of accomplishment and joy in the air. The final event was a spectacular fireworks display over the harbor, illuminating the night sky in a brilliant array of colors. Families gathered to watch, reflecting on the month of festivities and the sense of togetherness it had fostered.\n",
        "\n",
        "The 125th Festival of Lights in Cape Serenity was a resounding success, a testament to the town's enduring spirit and the power of tradition to bring people together. As the lights dimmed and the new year dawned, the residents of Cape Serenity looked forward to the future, proud of their heritage and excited for the many festivals yet to come.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LWCqiEHqRexR",
        "outputId": "5aa64b4b-2b83-4cce-ed17-342bbf13c81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using API Key from -> shopee-gmail-2\n",
            "## Summary of \"A Novel Double-Tail Generative Adversarial Network for Fast Photo Animation\"\n",
            "\n",
            "This research proposes a novel generative adversarial network (GAN) called DTGAN (Double-Tail GAN) for fast photo animation, meaning converting real-world photos into anime-style images. \n",
            "\n",
            "**Methodology Summary:**\n",
            "\n",
            "* **DTGAN Structure:** The generator in DTGAN has two output tails - a support tail for generating coarse-grained anime images and a main tail for refining them. \n",
            "* **Linearly Adaptive Denormalization (LADE):** A novel learnable normalization technique is introduced to prevent artifacts in generated images. LADE uses a point-wise convolution to obtain global data distribution information of features, guiding instance normalization and preventing artifacts.\n",
            "* **Loss Functions:**\n",
            "    * **Region Smoothing Loss:** Uses superpixel segmentation to weaken complex texture details in the generated images, achieving an abstract anime effect.\n",
            "    * **Fine-Grained Revision Loss:**  Eliminates artifacts and noise while preserving clear edges in the generated images.\n",
            "    * **Improved Grayscale Style Loss:**  Uses grayscale generated and anime images as inputs to avoid color interference in anime texture learning.\n",
            "    * **Improved Color Reconstruction Loss:**  Uses L1 loss in the Lab color space, which is closer to human vision, for better color preservation.\n",
            "* **Lightweight Generator:** The generator uses a lightweight attention module instead of a deep residual module for faster photo animation.\n",
            "* **Training:** DTGAN is trained end-to-end with unpaired training data in an unsupervised manner.\n",
            "\n",
            "**Results Summary:**\n",
            "\n",
            "* **Quantitative Evaluation:** DTGAN achieves the lowest FID and KID scores compared to state-of-the-art models, indicating generated images are closer to real anime images.\n",
            "* **Qualitative Evaluation:** DTGAN produces images with clearer edges, authentic colors, actual brightness, abstract details, and less noise compared to other models.\n",
            "* **Ablation Study:** LADE outperforms other normalization methods (BN, LN, IN, GN) in terms of visual quality and quantitative metrics (FID, KID, PSNR, SSIM). The proposed loss functions significantly improve the visual quality of generated images.\n",
            "\n",
            "**Important Aspects of the Method:**\n",
            "\n",
            "* **LADE:** A novel normalization technique that effectively avoids artifacts caused by IN, leading to higher quality anime images.\n",
            "* **Lightweight Generator:** Achieves faster photo animation compared to methods using deep residual modules.\n",
            "* **Loss Functions:** Effectively capture anime style and refine visual quality. The region smoothing loss prevents over-stylization, while the fine-grained revision loss eliminates artifacts and noise.\n",
            "\n",
            "**Citations for Each Argument:**\n",
            "\n",
            "* The proposed DTGAN architecture:  [19] Chen, J., Liu, G., & Chen, X. (2019). Animegan: A novel lightweight gan for photo animation. In *Proceedings of 11th International Symposium on Intelligence Computation and Applications* (ISICA’2019) (pp. 242-256). Guangzhou, China.\n",
            "* LADE: Equation (1) \n",
            "* Region Smoothing Loss: Equation (9)\n",
            "* Fine-Grained Revision Loss: Equation (16)\n",
            "* Improved Grayscale Style Loss: Equation (8)\n",
            "* Improved Color Reconstruction Loss: Equation (10) \n",
            "* Lightweight Generator:  [33] Guo, M., Liu, Z., Mu, T., & Hu, S. (2022). Beyond self-attention: External attention using two linear layers for visual tasks. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *01*(1), 1-13.\n",
            "* Quantitative Evaluation: [43] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). GANs trained by a two time-scale update rule converge to a local nash equilibrium. In *Proceedings of 31st Annual Conference on Neural Information Processing Systems* (NIPS’2017) (pp. ). Long Beach, CA, United States.\n",
            "\n",
            "**References:**\n",
            "\n",
            "[1] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In *Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition* (CVPR’2017) (pp. 5967-5976). Honolulu, HI, United States.\n",
            "[3] Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. In *Proceedings of 16th IEEE International Conference on Computer Vision* (ICCV’2017) (pp. 2242-2251). Venice, Italy.\n",
            "[5] Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A neural algorithm of artistic style. *CoRR abs/1508.06576*.\n",
            "[6] Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neural networks. In *Proceedings of 29th IEEE Conference on Computer Vision and Pattern Recognition* (CVPR’2016) (pp. 2414-2423). Las Vegas, NV, United States.\n",
            "[16] Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., & Yang, M. H. (2017). Diversified textures synthesis with feed-forward networks. In *Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition* (CVPR’2017) (pp. 266-274). Honolulu, HI, United States.\n",
            "[17] Chen, Y., Lai, Y. K., & Liu, Y. J. (2018). CartoonGAN: Generative adversarial networks for photo cartoonization. In *Proceedings of 31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (CVPR’2018) (pp. 9465-9474). Salt Lake City, UT, United States.\n",
            "[19] Chen, J., Liu, G., & Chen, X. (2019). Animegan: A novel lightweight gan for photo animation. In *Proceedings of 11th International Symposium on Intelligence Computation and Applications* (ISICA’2019) (pp. 242-256). Guangzhou, China.\n",
            "[21] Chen, X., & Liu, G. (2020). AnimeGANv2. [https://tachibanayoshino.github.io/AnimeGANv2](https://tachibanayoshino.github.io/AnimeGANv2). \n",
            "[29] Wu, H., Zheng, S., Zhang, J., & Huang, K. (2018). Fast end-to-end trainable guided filter. In *Proceedings of 31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (CVPR’2018) (pp. 1838-1847). Salt Lake City, UT, United States.\n",
            "[33] Guo, M., Liu, Z., Mu, T., & Hu, S. (2022). Beyond self-attention: External attention using two linear layers for visual tasks. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *01*(1), 1-13.\n",
            "[42] Agustsson, E., & Timofte, R. (2017). Ntire 2017 challenge on single image super-resolution: Dataset and study. In *Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition Workshops* (CVPRW’2017) (pp. 1122-1131). Honolulu, HI, United States.\n",
            "[43] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). GANs trained by a two time-scale update rule converge to a local nash equilibrium. In *Proceedings of 31st Annual Conference on Neural Information Processing Systems* (NIPS’2017) (pp. ). Long Beach, CA, United States.\n",
            "\n",
            "This is a concise summary of the key points in the paper. This summary focuses on providing researchers with the critical information they need, such as the key innovations in the methodology and the significant results, while providing relevant citations for further exploration. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = summarizer_gemini.run_gemini_summarizer(\n",
        "    text=\"/content/AnimeGANv3_manuscript.pdf\",\n",
        "    mode=\"pdf\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "idQlgLfdxjww",
        "outputId": "4858ccbe-ba6f-40fe-b68e-812c80a92ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using API Key from -> muhammad.nabiel11@ui.ac.id\n"
          ]
        }
      ],
      "source": [
        "result = topic_modeller.run_analysis(\n",
        "    text=\"/content/AnimeGANv3_manuscript.pdf\",\n",
        "    mode=\"pdf\", media=\"frontend\"\n",
        ")\n",
        "print(json.dumps(result, indent=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
